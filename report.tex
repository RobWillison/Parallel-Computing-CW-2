\documentclass{article}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{array}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{float}
\usepackage{listings}

\usepackage{listings}
\begin{document}

\title{CM30225 Parallel Computing \\ Assessed Courseork Assignment 1}
\author{}

\maketitle

\section{Approach}

In order to parallalise the relaxation problem we need to be able to split the matrix
up into chunks then let each node relax its own chunk with as little communication
between nodes as possible.\\~\\
In order to chunk the matrix up we want to give each node as similar numbers of
rows as possible to distribute work evenly. In my implementation first we calculate
the rounded up value of the number of rows minus 2, because the border rows arn't relaxed,
divided by the number of nodes. Each node is the allocated this many rows until
there are no rows left in the matrix. For example if we run 5 nodes on a 100 x 100 matrix
the rounded value is 20 so the first 4 nodes are allocated 20 and the last is allocated
18.\\~\\
The least each node needs to know after each iteration is the values in the row
to the left and right of its chunk. For example given a 6 x 6 matrix and 2 nodes,
the first is allocated row 2 and 3 and the second rows 4 and 5, after one iteration
all the first nodes needs is row 4 and the second node only needs row 3. Therefore
after each iteration each node sends its outside rows to the appropriate nodes.\\~\\
We can improve this by calculating the two outside rows before any others and sending
them, so the node can be calculating the rest of the chunk while the communication
happens. To do this MPI\_Isend and MPI\_Irecv are used as these are non blocking
so the node won't wait for the other node to receive the value before continuing.

\section{Testing}

To correctness test the program it was split into three separate chunks which would
be tested spectrally. The first is the ability to perform one iteration correctly and
replace all cells with the average of their four neighbors. The second is that
the iterations will stop after the precision is reached, and finally that when more
than one processor is used the output is the same.\\~\\
To test the first case the program was run on a 5x5 matrix, surrounded by 10's, for one iteration and the
answer captured. If the program works correctly the output should consist of 5
in each of the corners and 2.5 around the edge. As shown by the output below this is
working correctly.

\begin{center}
\begin{tabular}{ c c c c c }
 10 & 10 & 10 & 10 & 10 \\
 10 & 5.0 & 2.5 & 5.0 & 10 \\
 10 & 2.5 & 0.0 & 2.5 & 10 \\
 10 & 5.0 & 2.5 & 5.0 & 10 \\
 10 & 10 & 10 & 10 & 10 \\
\end{tabular}
\end{center}

The next step is too check that the iterations stop when the precision is met.
In order to test this the same 5x5 matrix was used. The precision was first set to
5 this should yield one iteration as the precision is met immediately, the precision was
then halved to 2.5 which should yield 2 iterations. As shown by the result table below
this worked as expected.

\begin{center}
\begin{tabular}{ c c c c }
iteration & diffrence & precision & continue \\
 1 & 5 & 5 & 0\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ c c c c }
iteration & diffrence & precision & continue \\
 1 & 5 & 2.5 & 1\\
 2 & 2.5 & 2.5 & 0\\
\end{tabular}
\end{center}

Finally to test that these conditions still hold when multiple threads are used
a 10x10 matrix was run using 1, 2 and 4 processors and each where checked to make
sure they were identical. below are the outputs from the runs with 1 and 4 processors.
\newpage

\begin{table}[]
\caption{1 Processor}
\begin{tabular}{llllllllll}
10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 \\
10.000 & 9.981  & 9.965  & 9.953  & 9.946  & 9.946  & 9.953  & 9.965  & 9.981  & 10.000 \\
10.000 & 9.965  & 9.934  & 9.911  & 9.899  & 9.899  & 9.911  & 9.934  & 9.965  & 10.000 \\
10.000 & 9.953  & 9.911  & 9.881  & 9.864  & 9.864  & 9.881  & 9.911  & 9.953  & 10.000 \\
10.000 & 9.946  & 9.899  & 9.864  & 9.846  & 9.846  & 9.864  & 9.899  & 9.946  & 10.000 \\
10.000 & 9.946  & 9.899  & 9.864  & 9.846  & 9.846  & 9.864  & 9.899  & 9.946  & 10.000 \\
10.000 & 9.953  & 9.911  & 9.881  & 9.864  & 9.864  & 9.881  & 9.911  & 9.953  & 10.000 \\
10.000 & 9.965  & 9.934  & 9.911  & 9.899  & 9.899  & 9.911  & 9.934  & 9.965  & 10.000 \\
10.000 & 9.981  & 9.965  & 9.953  & 9.946  & 9.946  & 9.953  & 9.965  & 9.981  & 10.000 \\
10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 \\
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{4 Processors}
\begin{tabular}{llllllllll}
10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 \\
10.000 & 9.981  & 9.965  & 9.953  & 9.946  & 9.946  & 9.953  & 9.965  & 9.981  & 10.000 \\
10.000 & 9.965  & 9.934  & 9.911  & 9.899  & 9.899  & 9.911  & 9.934  & 9.965  & 10.000 \\
10.000 & 9.953  & 9.911  & 9.881  & 9.864  & 9.864  & 9.881  & 9.911  & 9.953  & 10.000 \\
10.000 & 9.946  & 9.899  & 9.864  & 9.846  & 9.846  & 9.864  & 9.899  & 9.946  & 10.000 \\
10.000 & 9.946  & 9.899  & 9.864  & 9.846  & 9.846  & 9.864  & 9.899  & 9.946  & 10.000 \\
10.000 & 9.953  & 9.911  & 9.881  & 9.864  & 9.864  & 9.881  & 9.911  & 9.953  & 10.000 \\
10.000 & 9.965  & 9.934  & 9.911  & 9.899  & 9.899  & 9.911  & 9.934  & 9.965  & 10.000 \\
10.000 & 9.981  & 9.965  & 9.953  & 9.946  & 9.946  & 9.953  & 9.965  & 9.981  & 10.000 \\
10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000 & 10.000
\end{tabular}
\end{table}

Given these results it is concluded that the program correctly calculates the relaxation
of a matrix with any number of processors.

\section{Scalability Investigation}

\subsection{Speedup}

The speedup was calculated using a 10,000 by 10,000 matrix using a selection of
nodes between 1 and 64. The results are given in the table below.

\begin{center}
\pgfplotstabletypeset[
  columns/nodes/.style={column name=processors},
  columns/time/.style={column name=Time (S)},
  columns/speedup/.style={column name=Speedup on P processors},
]{data/speedup}
\end{center}

As the results show the speedup on P processors is very close to the number of processors
used. This is true until over 16 processors, at this point the extra communication
overhead added by using another processor starts to outweigh the extra processing
power gained. Figure 1 shows this in graph form.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Processors},
     ylabel=Speedup,
     ]
   \addplot table [x=nodes,y=speedup] {data/speedup};
 \end{axis}
 \end{tikzpicture}
 \caption{Speedup}
 \label{fig:speedup}
 \end{figure}

\subsection{Amdahls Limit}

Amdahls Limit is the maximum limit speedup can reach, this is because a certain ammount
of the computation must be done in serial and therefor adding more processors will not
reduce the time this takes.\\
The only parts of the system that have to run in parallel is the checking if all the
nodes have finished and the rebuilding of the matrix at the end of the computation.
However the creation of the matrix and the swapping of the read and write matrix
happens on all processors so adding more will not decrease this time. Also something
to note is as the number of processors increases the amount of time taken to check
if each processor has done and rebuild the matrix will increase as there are more
processors to communicate with.\\
Due to the fact that the serial parts of the problem increase as the number of processors
are added the theoretical amdahl limit was not calculated.

\subsection{Slowdown}

When smalled matrix's were used it was noticed that as more threads were added the
time time taken to complete would increase. This is due to the overhead of communicating
between processors is more than the time saved by splitting the problem up further.
Slowdown can be observed when a 1000 by 1000 matrix is used, results are show in Figure 2
below.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Processors},
     ylabel=Time (S),
     ]
   \addplot table [x=nodes,y=time] {data/TimeThreads1000};
 \end{axis}
 \end{tikzpicture}
 \caption{Slowdown}
 \label{fig:slowdown}
 \end{figure}

As Figure 2 shows when more that around 16 processors were used the amount of time
required to complete the computation increased.

\subsection{Gustafson's Law}

Gustafson's Law states that if you increase the size of the problem then the amount of
time spent on any sequential parts will be a less significant chunk of overall runtime.
This means better speedup values can be obtained. To show this the speedup values were
calculated for differing sizes of matrices to show as the size increase so does
speedup.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Matrix Size},
     ylabel=Speedup,
     ]
   \addplot table [x=size,y=speedup] {data/gustafsons8};
 \end{axis}
 \end{tikzpicture}
 \caption{Gustafson's Law 8 Processors}
 \label{fig:slowdown}
 \end{figure}

 \begin{figure}[H]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
      xlabel={Matrix Size},
      ylabel=Speedup,
      ]
      \addplot table [x=size,y=speedup] {data/gustafsons64};
  \end{axis}
  \end{tikzpicture}
  \caption{Gustafson's Law 64 Processors}
  \label{fig:slowdown}
  \end{figure}

  As you can see from Figure 3 and 4 as Gustafson's Law suggests as the size of
  the problem is increased the speed up on P processors also increases, until the
  speedup reaches the number of processors used.

  \subsection{Efficiency}

The efficiency was calculated for a number of processors and problem sizes, the results
are shown in Figure 5.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Processors},
     ylabel=Efficiency,
     ]
     \addplot table [x=processor,y=efficiency] {data/efficiency1000};
     \addplot table [x=processor,y=efficiency] {data/efficiency10000};
     \addlegendentry{1000 x 1000 matrix}
     \addlegendentry{10000 x 10000 matrix}
 \end{axis}
 \end{tikzpicture}
 \caption{Efficiency}
 \label{fig:slowdown}
 \end{figure}

As expected from the results of the speedup investigation earlier the efficiency drops
as more processors are added and the problem size is kept constant. This is because
as some of the program has to be done in serial for that part every other processor
is idle, therefore if more processors are added the cumulative time a processor is ideal increases.

\subsection{Karp-Flatt}

The Karp-Flatt metric is a measure for the sequential part of the program, the larger
the Karp-Flatt metric the larger the sequential part of the problem. The metric was
calculated for three different matrix sizes on different numbers of processors, it
is expected that as more processors are added the Karp-Flatt will increase due to
the longer time spent on communication. The results are shown in Figure 6.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Processors},
     ylabel=KarpFlatt,
     ]
   \addplot table [x=processors,y=karpflatt] {data/karpflatt1000};
   \addplot table [x=processors,y=karpflatt] {data/karpflatt5000};
   \addplot table [x=processors,y=karpflatt] {data/karpflatt10000};
   \addlegendentry{1000 x 1000}
   \addlegendentry{5000 x 000}
   \addlegendentry{10000 x 10000}
 \end{axis}
 \end{tikzpicture}
 \caption{0.01 precision}
 \label{fig:karpflatt}
\end{figure}

As the graph shows what we expected is true, as the number of processors is increased
the Karp-Flatt metric also increases. The increase on smaller problem sizes is much
larger than those on larger problems. This is because the proportion of the time
spent doing sequential tasks is much larger on smaller problem sizes.

\subsection{Parallel Overhead}

The overhead was calculated to evaluate the work efficiency of the parallel program.
The equation below was used to calculate the overhead.

\begin{center}
$Overhead\ =\ p\ \times\ time\ on\ P\ processors\ -\ time\ on\ 1\ processor}$
\end{center}

This gives the total parallel overhead for a given number of processors in seconds.
This total efficiency was divided by the number of processors to give the overhead
per thread in seconds. The results from this are shown in Figure 7.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Processors},
     ylabel=Overhead per Processor (S),
     ]
   \addplot table [x=processors,y=overhead] {data/overhead1000};
   \addplot table [x=processors,y=overhead] {data/overhead5000};
   \addplot table [x=processors,y=overhead] {data/overhead10000};
   \addlegendentry{1000 x 1000}
   \addlegendentry{5000 x 000}
   \addlegendentry{10000 x 10000}
 \end{axis}
 \end{tikzpicture}
 \caption{0.01 precision}
 \label{fig:overhead}
\end{figure}

As shown in figure 7 as more processors are added the parallel overhead per thread
increases. This is expected as when more processors are added there is more communication
needed. Also as the problem size is increased the overhead also increases this is due
to the fact there is more data to transfer between processors thus communication times increased.

\subsection{Isoefficiency}

The isoefficiency was calculated in order to work out how well the program scales.
The isoefficiency tells you how much the problem size has to increased by in order
to  keep the efficiency constant. In order to work out the isoefficiency an efficiency
of 0.8 was chosen and the number of cores needed to reach this efficiency
for different problem sizes was found. These results are shown in the table below.

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
  Problem size & 0.8 efficiency\\
  1000 & 8\\
  3000 & 16\\
  5000 & 21\\
  7000 & 26\\
  9000 & 30\\
 \hline
\end{tabular}
\end{center}

The results in the table are shown in graphical form in Figure 8.

\begin{figure}[H]
 \centering
 \begin{tikzpicture}
 \begin{axis}[
     xlabel={Matrix Size},
     ylabel=Processors,
     ]
   \addplot table [x=size,y=eight] {data/isoefficiency};
   \addlegendentry{0.8 efficiency}
 \end{axis}
 \end{tikzpicture}
 \caption{}
 \label{fig:Isoefficiency}
\end{figure}

A line of best fit can be drawn through those points with an equation of
$y=0.0027x+6.7$. This shows that to maintain a given efficiency the matrix
size must be increased by approximately 350 in each direction when another processor is added.

\subsection{Conclusion}

In conclusion the scalability is very good for the implementation given. The main
reason for this is that the isoefficiency has a linear relationship and that if a
new processor is added the matrix size only needs to be increased by 350 in both
directions to maintain efficiency.


\end{document}
